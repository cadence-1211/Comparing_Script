Of course. Let's walk through this entire process from start to finish. I will give you all the code and a simple, step-by-step guide.

Think of this process like a big cooking project. You have too many ingredients (lines in your files) to cook in one pot.

The Plan (Our Strategy):

Step 1: Chop the Ingredients. We'll first write a script to "chop up" your two giant files into 5 smaller, matched pairs. We'll call this the Sharder.

Step 2: Cook in Parallel. We'll send these 5 pairs of smaller files to 5 different "stoves" (machines on your LSF cluster) to be cooked (compared) at the same time.

Step 3: Combine for the Final Meal. Once each stove is done, we'll take the 5 small finished dishes and combine them into one big, final result.

You only need to run three commands in total. I will explain each one.

The Complete Guide for Parallel Comparison
Step 1: Get Your Files Ready

First, let's get organized.

On your Linux machine, create a new folder for this project. Let's call it MyComparisonProject.

Inside MyComparisonProject, place your two giant files: file1.txt and file2.txt.

Inside this same MyComparisonProject folder, you will create the three scripts below. Just copy and paste the code I provide into new files with the exact names I specify.

Step 2: Create the Three Scripts

Create these three files inside your MyComparisonProject folder.

This script reads your big files and splits them into smaller, matched chunks.

Create a file named sharder.py and paste this code into it:

Generated python
# sharder.py
import os
import argparse

def get_instance_key(line, key_cols):
    """Extracts the key from a line."""
    parts = line.strip().split()
    if len(parts) <= max(key_cols):
        return None
    return "_".join(parts[i] for i in key_cols)

def shard_file(input_file, key_cols, num_shards, output_dir):
    """Reads a large file and splits it into smaller shards based on a key."""
    print(f"Processing {input_file}...")
    
    # Create file handles for all the output shard files
    output_files = [open(os.path.join(output_dir, f"{os.path.basename(input_file)}_shard_{i}.txt"), "w") for i in range(num_shards)]
    
    with open(input_file, "r", errors='ignore') as f:
        for line in f:
            # Skip empty or comment lines
            if not line.strip() or line.strip().startswith("#"):
                continue
            
            key = get_instance_key(line, key_cols)
            if key is None:
                continue

            # This is the magic: it calculates which file (0, 1, 2, 3, or 4) this line should go to.
            # The hash() function ensures the same key always goes to the same file number.
            shard_index = hash(key) % num_shards
            
            # Write the line to the correct shard file
            output_files[shard_index].write(line)
            
    # Close all the shard files
    for file_handle in output_files:
        file_handle.close()
    print(f"Finished sharding {input_file}.")


def main():
    parser = argparse.ArgumentParser(description="Shard large files based on instance keys.")
    parser.add_argument("--file1", required=True, help="Path to the first large file.")
    parser.add_argument("--instcol1", required=True, help="Comma-separated key columns for file 1.")
    parser.add_argument("--file2", required=True, help="Path to the second large file.")
    parser.add_argument("--instcol2", required=True, help="Comma-separated key columns for file 2.")
    parser.add_argument("--shards", type=int, default=5, help="Number of shards to create.")
    args = parser.parse_args()

    # Create a directory to hold the shards
    output_dir = "shards"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    print(f"Created output directory: '{output_dir}'")
    
    instcol1 = list(map(int, args.instcol1.split(',')))
    instcol2 = list(map(int, args.instcol2.split(',')))
    
    shard_file(args.file1, instcol1, args.shards, output_dir)
    shard_file(args.file2, instcol2, args.shards, output_dir)

    print("\nSharding complete!")

if __name__ == "__main__":
    main()


This is the main Python script from before, with one tiny change to save results with unique names.

Create a file named compare_adv.py and paste this code into it:

Generated python
# compare_adv.py
import argparse
import os
import time
import sys
import mmap
import csv
import multiprocessing
import re

# MODIFICATION: Added an argument for output prefix
def main():
    parser = argparse.ArgumentParser(description="Compare two files, with user-defined keys and advanced value comparison.")
    parser.add_argument("--file1", help="Path to the first file.")
    parser.add_argument("--instcol1", help="Comma-separated 0-based index(es) for instance key columns in file 1.")
    parser.add_argument("--valcol1", type=int, help="0-based index for the value column in file 1.")
    parser.add_argument("--file2", help="Path to the second file.")
    parser.add_argument("--instcol2", help="Comma-separated 0-based index(es) for instance key columns in file 2.")
    parser.add_argument("--valcol2", type=int, help="0-based index for the value column in file 2.")
    # THIS IS THE ONLY NEW ARGUMENT
    parser.add_argument("--output_prefix", default="result", help="Prefix for output files (e.g., 'run_0').")
    
    args = parser.parse_args()
    
    # The rest of the script is IDENTICAL to the one you already have.
    # We just use the 'output_prefix' when saving files.
    
    if not args.file1 or not args.instcol1 or args.valcol1 is None or not args.file2 or not args.instcol2 or args.valcol2 is None:
        print("This script is intended for non-interactive LSF runs. All arguments must be provided.")
        print("Required: --file1, --instcol1, --valcol1, --file2, --instcol2, --valcol2")
        sys.exit(1)

    instcol1 = list(map(int, args.instcol1.strip().split(",")))
    instcol2 = list(map(int, args.instcol2.strip().split(",")))

    comparison_type = 'numeric' # Assuming numeric for batch jobs, or you could pass it as an arg

    if len(instcol1) != len(instcol2):
        print("❌ Error: The number of instance key columns must be the same for both files.")
        sys.exit(1)

    t0 = time.time()
    file1_name = os.path.basename(args.file1)
    file2_name = os.path.basename(args.file2)

    with multiprocessing.Pool(2) as pool:
        results = pool.map(parse_file_worker, [
            (args.file1, instcol1, args.valcol1, comparison_type),
            (args.file2, instcol2, args.valcol2, comparison_type)
        ])
    
    data1, instances1 = results[0]
    data2, instances2 = results[1]
    
    miss2, miss1, matched = compare_instances(data1, data2, instances1, instances2)

    # MODIFICATION: Using the prefix for output filenames
    missing_filename = f"{args.output_prefix}_missing_instances.txt"
    comparison_filename = f"{args.output_prefix}_comparison.csv"
    
    write_missing_file(file1_name, file2_name, miss2, miss1, missing_filename)
    write_comparison_csv(file1_name, file2_name, data1, data2, matched, "Value1", "Value2", comparison_filename)
    
    t1 = time.time()
    print(f"Run {args.output_prefix} finished in {t1 - t0:.2f} seconds.")
    print(f"Results saved to {missing_filename} and {comparison_filename}")


# --- Helper Functions (No changes needed in these) ---
NUMERIC_RE = re.compile(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?")
METADATA_KEYWORDS_SET = {b"VERSION", b"CREATION", b"CREATOR", b"PROGRAM", b"DIVIDERCHAR", b"DESIGN", b"UNITS", b"INSTANCE_COUNT", b"NOMINAL_VOLTAGE", b"POWER_NET", b"GROUND_NET", b"WINDOW", b"RP_VALUE", b"RP_FORMAT", b"RP_INST_LIMIT", b"RP_THRESHOLD", b"RP_PIN_NAME", b"MICRON_UNITS", b"INST_NAME"}

def is_valid_instance_line(line):
    line = line.strip()
    if not line or line.startswith(b"#"): return False
    for keyword in METADATA_KEYWORDS_SET:
        if line.startswith(keyword): return False
    return True

def extract_value(value_bytes, comparison_type):
    value_str = value_bytes.decode('utf-8', errors='ignore').strip()
    if comparison_type == 'numeric':
        match = NUMERIC_RE.search(value_str)
        if match:
            try: return float(match.group(0))
            except (ValueError, TypeError): return value_str
        else: return value_str
    else: return value_str

def parse_file_with_mmap(file_path, inst_cols, value_col, comparison_type):
    data, instances_set = {}, set()
    with open(file_path, "rb") as f:
        mmapped_file = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
        for line in iter(mmapped_file.readline, b""):
            if not is_valid_instance_line(line): continue
            parts = line.strip().split()
            if len(parts) <= max(inst_cols + [value_col]): continue
            try:
                key = tuple(parts[i].decode('utf-8', errors='ignore').strip() for i in inst_cols)
                val_raw = parts[value_col].decode('utf-8', errors='ignore').strip()
                val_parsed = extract_value(parts[value_col], comparison_type)
                data[key] = (val_raw, val_parsed)
                instances_set.add(key)
            except IndexError: continue
        mmapped_file.close()
    return data, instances_set

def compare_instances(data1, data2, instances1, instances2):
    missing_in_file2 = sorted([i for i in instances1 if i not in instances2])
    missing_in_file1 = sorted([i for i in instances2 if i not in instances1])
    matched = sorted(list(instances1 & instances2))
    return missing_in_file2, missing_in_file1, matched

def write_missing_file(file1_name, file2_name, miss2, miss1, out_filename):
    with open(out_filename, "w") as out:
        if miss2:
            out.writelines([f"{'='*60}\n", f"Instances from '{file1_name}' missing in '{file2_name}':\n", f"{'='*60}\n"])
            out.writelines(f"{' | '.join(inst)}\n" for inst in miss2)
        if miss1:
            out.writelines([f"\n{'='*60}\n", f"Instances from '{file2_name}' missing in '{file1_name}':\n", f"{'='*60}\n"])
            out.writelines(f"{' | '.join(inst)}\n" for inst in miss1)

def write_comparison_csv(file1_name, file2_name, data1, data2, matched, col_name1, col_name2, out_filename):
    if not matched: return
    with open(out_filename, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        key_len = len(matched[0]) if matched else 1
        headers = [f"Instance_Key_{i+1}" for i in range(key_len)] + [f"{file1_name}_{col_name1}", f"{file2_name}_{col_name2}", "Difference", "Result"]
        writer.writerow(headers)
        for inst in matched:
            raw1, val1 = data1.get(inst, (None, None))
            raw2, val2 = data2.get(inst, (None, None))
            if isinstance(val1, float) and isinstance(val2, float):
                diff = val1 - val2
                if val2 != 0:
                    deviation = abs((diff / val2) * 100)
                    result = f"{deviation:.2f}%"
                else:
                    result = "Infinite %"
                writer.writerow(list(inst) + [f"{val1:.4f}", f"{val2:.4f}", f"{diff:.4f}", result])
            else:
                match_result = "MATCH" if str(val1) == str(val2) else "MISMATCH"
                writer.writerow(list(inst) + [str(val1), str(val2), "N/A", match_result])

def parse_file_worker(args_tuple):
    return parse_file_with_mmap(*args_tuple)

if __name__ == "__main__":
    main()
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

This is a simple script that tells LSF to run our comparator 5 times, once for each pair of chopped-up files.

Create a file named run_lsf.sh and paste this code into it:

Generated bash
#!/bin/bash
# run_lsf.sh

echo "Submitting 5 comparison jobs to LSF..."

# Create a directory for the results and logs
mkdir -p results
mkdir -p logs

# This loop runs 5 times (for 0, 1, 2, 3, 4)
for i in {0..4}
do
  # This is the LSF command
  # It tells the cluster to run our python script on each pair of shards
  bsub -n 2 -R "rusage[mem=8G]" -o "logs/output_${i}.log" \
    python3 compare_adv.py \
      --file1 "shards/file1.txt_shard_${i}.txt" \
      --file2 "shards/file2.txt_shard_${i}.txt" \
      --instcol1 0 \
      --valcol1 3 \
      --instcol2 0 \
      --valcol2 3 \
      --output_prefix "results/run_${i}"

  echo "  -> Submitted job for shard ${i}"
done

echo ""
echo "All jobs submitted! Check status with 'bjobs'."
echo "Once all jobs are 'DONE', you can merge the results."
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Important: In the script above, you might need to change the column numbers. I have used --instcol1 0 (first column) and --valcol1 3 (fourth column) as an example. Change these numbers to match your actual files.

Step 3: Run Everything with Just Three Commands

Open a terminal in your MyComparisonProject folder and run these commands one by one.

This command runs the sharder script. It will read file1.txt and file2.txt and create a new folder called shards containing 10 smaller files.

Generated bash
python3 sharder.py --file1 file1.txt --instcol1 0 --file2 file2.txt --instcol2 0
```*(Again, change `--instcol1 0` if your instance key is in a different column)*.

##### Command 2: Run the comparison on the cluster
This command tells LSF to start the 5 parallel jobs.

```bash
sh run_lsf.sh
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Now, the LSF cluster is doing the hard work. You can check the progress by typing the command bjobs. Wait until bjobs shows that all your jobs are DONE. This might take some time.

Once all jobs are finished, you will have a results folder full of partial CSV files. This final command will stitch them all together into one master file.

Copy and paste this entire block of commands into your terminal and press Enter:

Generated bash
echo "Merging final result files..."

# For the CSV file: copy the first file, then add the rest without their headers
cat results/run_0_comparison.csv > final_comparison.csv
for i in {1..4}
do
  tail -n +2 results/run_${i}_comparison.csv >> final_comparison.csv
done

# For the missing instances file: just combine them all
cat results/run_*_missing_instances.txt > final_missing_instances.txt

echo "Done! Your final files are:"
echo "  -> final_comparison.csv"
echo "  -> final_missing_instances.txt"
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

That's it! You will now have your two final result files, final_comparison.csv and final_missing_instances.txt, in your MyComparisonProject folder.
